var firstDecimal = 1.23
var secondDecimal = 3.21
//:  Declare a variable called `trueOrFalse` and give it a boolean value. Try to assign it to `firstDecimal` like so: `firstDecimal = trueOrFalse`. Does it compile? Print a statement to the console explaining why not, and remove the line of code that will not compile.
var trueOrfalse = false
// firstDecimal = trueOrfalse
print("The code will not compile due to the values being different types")
//:  Declare a variable and give it a string value. Then try to assign it to `firstDecimal`. Does it compile? Print a statement to the console explaining why not, and remove the line of code that will not compile.
var sayings = "Break a leg!"
// firstDecimal = sayings
print("This code will not compile due to the values being two different types")
//:  Finally, declare a variable with a whole number value. Then try to assign it to `firstDecimal`. Why won't this compile even though both variables are numbers? Print a statement to the console explaining why not, and remove the line of code that will not compile.
var minute = 8
// firstDecimal = minute
print("Thsi code wont compile because they are viewed as different types although they are both numbers")

/*:
[Previous](@previous)  |  page 7 of 10  |  [Next: App Exercise - Tracking Different Types](@next)
 */
